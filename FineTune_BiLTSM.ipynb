{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26667,"status":"ok","timestamp":1678996624074,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"cJ5JWuK17Qnq","outputId":"722a9a85-e043-4465-d8ee-c9d1bbf8ac29"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1678996624076,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"PEEFeuY8xDWs","outputId":"092588aa-2eea-43ff-8977-20da32d25f2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/CW2Code\n"]}],"source":["cd /content/drive/MyDrive/CW2Code/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96027,"status":"ok","timestamp":1679002901721,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"186uKcxswIld","outputId":"1fd869e7-5314-4fc1-cd14-1c8b7dbb1923"},"outputs":[{"name":"stdout","output_type":"stream","text":["replace glove.twitter.27B.25d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","replace glove.twitter.27B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","replace glove.twitter.27B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"]}],"source":["# !unzip -q \"glove_twitter.zip\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlkfjlYYwet6"},"outputs":[],"source":["# !unzip -q \"Sentiment140.zip\""]},{"cell_type":"markdown","metadata":{"id":"X9kifkiUBoG6"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22238,"status":"ok","timestamp":1678996650604,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"4QDoTYc9zRqo","outputId":"34c048b3-ad87-4aff-b32c-24e024b13a4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting pyahocorasick\n","  Downloading pyahocorasick-2.0.0-cp39-cp39-win_amd64.whl (39 kB)\n","Collecting anyascii\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n","Collecting emoji\n","  Downloading emoji-2.2.0.tar.gz (240 kB)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py): started\n","  Building wheel for emoji (setup.py): finished with status 'done'\n","  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234925 sha256=17570ee5b0c42e597f48fad90fbbd0f9470c5db9da07faaf61c960b2115451a6\n","  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\9a\\b8\\0f\\f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-2.2.0\n","Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","Installing collected packages: emot\n","Successfully installed emot-3.1\n","Requirement already satisfied: tensorflow in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.8.0)\n","Requirement already satisfied: keras in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.8.0)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.44.0)\n","Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n","Collecting numpy>=1.20\n","  Downloading numpy-1.24.2-cp39-cp39-win_amd64.whl (14.9 MB)\n","Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n","Requirement already satisfied: h5py>=2.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.24.0)\n","Requirement already satisfied: libclang>=9.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (13.0.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: gast>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.20.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.28.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","Successfully installed numpy-1.24.2\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n","scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.2 which is incompatible.\n","numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.24.2 which is incompatible.\n"]}],"source":["!pip install contractions\n","!pip install emoji --upgrade\n","!pip install emot\n","!pip install tensorflow keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1679007015293,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"RgMK1Ex6C_fm","outputId":"c7f4a5ef-1903-4efd-bd25-388bec3b6026"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import contractions\n","import pandas as pd\n","import emoji\n","import re\n","import emot\n","import string\n","import codecs\n","import numpy as np\n","import html\n","from tqdm import tqdm\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import pickle\n","\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Input, Dropout, Flatten\n","from tensorflow.keras.models import load_model"]},{"cell_type":"markdown","metadata":{"id":"e1sGaTkVBuJX"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5WNMx9004Ja"},"outputs":[],"source":["columns  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmJLyeLr08jg"},"outputs":[],"source":["dataset = pd.read_csv('./training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\" , names=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T37zXPYf1xWQ"},"outputs":[],"source":["# Drop irrelavant columns from dataset dataframe\n","dataset = dataset.drop(['ids', 'date','flag','user'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lF8Etg38CGB2"},"outputs":[],"source":["dataset['sentiment'] = dataset['sentiment'].replace(4,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1678997249686,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"e6P2LwbUCISf","outputId":"03588563-3ed6-4522-b869-4b954a70066c"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-44682a83-bfd2-4b55-b664-8bc157edc131\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>1</td>\n","      <td>Just woke up. Having no school is the best fee...</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>1</td>\n","      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>1</td>\n","      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>1</td>\n","      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n","    </tr>\n","    <tr>\n","      <th>1599999</th>\n","      <td>1</td>\n","      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1600000 rows Ã— 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44682a83-bfd2-4b55-b664-8bc157edc131')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-44682a83-bfd2-4b55-b664-8bc157edc131 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-44682a83-bfd2-4b55-b664-8bc157edc131');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         sentiment                                               text\n","0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1                0  is upset that he can't update his Facebook by ...\n","2                0  @Kenichan I dived many times for the ball. Man...\n","3                0    my whole body feels itchy and like its on fire \n","4                0  @nationwideclass no, it's not behaving at all....\n","...            ...                                                ...\n","1599995          1  Just woke up. Having no school is the best fee...\n","1599996          1  TheWDB.com - Very cool to hear old Walt interv...\n","1599997          1  Are you ready for your MoJo Makeover? Ask me f...\n","1599998          1  Happy 38th Birthday to my boo of alll time!!! ...\n","1599999          1  happy #charitytuesday @theNSPCC @SparksCharity...\n","\n","[1600000 rows x 2 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"markdown","metadata":{"id":"AyociNMdz5YD"},"source":["# Data PreProcessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBd1poAi70io"},"outputs":[],"source":["def remove_urls(tweet):\n","    # Removal of urls since may not be relevant to the content for analysis\n","    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","\n","    return url_pattern.sub(r'', tweet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fclTmfQc79-S"},"outputs":[],"source":["def clean_mentions(tweet):\n","    # Remove twitter mentions i.e. username since may not be relevant to the content and\n","    # thus extraneous information\n","    return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\",'',tweet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwLNI_UACwAj"},"outputs":[],"source":["def expand_contractions(tweet):\n","    # Expand shortened version of phrases or words\n","    tweet = contractions.fix(tweet)\n","\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQjvhWw1DREb"},"outputs":[],"source":["def replace_numbers(tweet):\n","    # Removal of numbers from tweet\n","    return re.sub('[0-9]+', '', tweet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cboatUGsXvfM"},"outputs":[],"source":["slang_dict = {\n","    'af': 'as fuck',\n","    'afaik': 'as far as I know',\n","    'amirite': 'am I right',\n","    'b4': 'before',\n","    'bc': 'because',\n","    'b/c': 'because',\n","    'btw': 'by the way',\n","    'brb': 'be right back',\n","    'cya': 'see ya',\n","    'da': 'the',\n","    'deets': 'details',\n","    'dope': 'cool',\n","    'fam': 'family',\n","    'fb': 'Facebook',\n","    'fml': 'fuck my life',\n","    'ftw': 'for the win',\n","    'gf': 'girlfriend',\n","    'gj': 'good job',\n","    'gr8': 'great',\n","    'h8': 'hate',\n","    'hmu': 'hit me up',\n","    'idc': 'I donâ€™t care',\n","    'idk': 'I donâ€™t know',\n","    'ikr': 'I know right',\n","    'imho': 'in my humble opinion',\n","    'imo': 'in my opinion',\n","    'irl': 'in real life',\n","    'j/k': 'just kidding',\n","    'jk': 'just kidding',\n","    'k': 'okay',\n","    'lmao': 'laughing my ass off',\n","    'lmk': 'let me know',\n","    'lol': 'laugh out loud',\n","    'm8': 'mate',\n","    'nbd': 'no big deal',\n","    'ngl': 'not gonna lie',\n","    'nm': 'nevermind',\n","    'np': 'no problem',\n","    'nsfw': 'not safe for work',\n","    'omg': 'oh my god',\n","    'ppl': 'people',\n","    'probs': 'probably',\n","    'qt': 'cutie',\n","    'rly': 'really',\n","    'rofl': 'rolling on the floor laughing',\n","    'smh': 'shaking my head',\n","    'sry': 'sorry',\n","    'stfu': 'shut the fuck up',\n","    'tbh': 'to be honest',\n","    'tho': 'though',\n","    'thx': 'thanks',\n","    'tmi': 'too much information',\n","    'ttyl': 'talk to you later',\n","    'u': 'you',\n","    'ur': 'your',\n","    'w/': 'with',\n","    'w/o': 'without',\n","    'wbu': 'what about you',\n","    'wtf': 'what the fuck',\n","    'wyd': 'what are you doing',\n","    'y': 'why',\n","    'yolo': 'you only live once',\n","    'yr': 'year'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyGrg_soX1gT"},"outputs":[],"source":["def replace_slangs_words(tweet, slang_dict):\n","    # Replace slag words as abbreviation to its corresponding english phrase\n","    tweet_wo_slangs = []\n","    for word in tweet.split():\n","        if word in slang_dict:\n","            tweet_wo_slangs.append(slang_dict[word])\n","        else:\n","            tweet_wo_slangs.append(word)\n","\n","    return ' '.join(tweet_wo_slangs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46_onqwmrkIi"},"outputs":[],"source":["nonstandard_dict = {\n","    'aint': 'am not',\n","    'gonna': 'going to',\n","    'gotta': 'got to',\n","    'hafta': 'have to',\n","    'wanna': 'want to',\n","    'coulda': 'could have',\n","    'woulda': 'would have',\n","    'shoulda': 'should have',\n","    'mighta': 'might have',\n","    'musta': 'must have',\n","    'ain\\'t': 'am not',\n","    'kinda': 'kind of',\n","    'sorta': 'sort of'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8vAvhy5rxMO"},"outputs":[],"source":["def replace_nonstandard(tweet, nonstandard_dict):\n","    # To replace slag words which are non-english words to their corresponding english words\n","    tweet_wo_nonstandard = []\n","    for word in tweet.split():\n","        if word in nonstandard_dict:\n","            tweet_wo_nonstandard.append(nonstandard_dict[word])\n","        else:\n","            tweet_wo_nonstandard.append(word)\n","\n","    return ' '.join(tweet_wo_nonstandard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXU36saFDU0o"},"outputs":[],"source":["def convert_emojis(tweet):\n","  # convert emoji to words to capture intended meaning of emoji\n","  tweet = emoji.demojize(tweet)\n","  tweet = re.sub('_',' ',tweet)\n","\n","  return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-G73VxwEjr0"},"outputs":[],"source":["def convert_emoticons(tweet):\n","    # convert emoticons to words to capture intended meaning of emoticons\n","    emo_obj = emot.EMOTICONS_EMO\n","    for emo in emo_obj:\n","        escaped_emot = re.escape(emo)\n","        tweet = re.sub(u'({})'.format(escaped_emot), \" \".join(emo_obj[emo].replace(\",\",\"\").split()), tweet)\n","\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bA4blL8-E0Ez"},"outputs":[],"source":["punctuations = string.punctuation\n","def remove_punctuations(tweet):\n","    # Removal of punctuations from tweets\n","    return tweet.translate(str.maketrans('', '', punctuations))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhOX2bj9x1z7"},"outputs":[],"source":["def load_file(path):\n","    file_text = ''\n","    file = codecs.open(path, 'r', encoding = 'utf-8')\n","    file_lines = file.readlines()\n","    for line in file_lines:\n","        # Text cleaning, remove any whitespace lines\n","        line = line.replace('\\n','')\n","        file_text = file_text + line\n","    file.close()\n","\n","    return file_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0_sBeuwxfum"},"outputs":[],"source":["# loads contents of a stopwords file\n","StopWords = load_file('./Stopwords.txt')\n","StopWords_list = re.split(r\"[~\\r\\n]+\", StopWords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwfT03piFCPh"},"outputs":[],"source":["def remove_stopwords(tweet):\n","    # Removal of stopwords from tweets which have no or less meaning to reduce noise\n","    return \" \".join([word for word in str(tweet).split() if word not in StopWords_list])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXActjY3MUjI"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n","def lemmatize_words(tweet):\n","  # Tag each word in the tweet with its corresponding part-of-speech.\n","  pos_tagged_tweet = nltk.pos_tag(tweet.split())\n","  # lemmatizer is applied to each word in the tweet using the WordNet POS tag. If the POS is not found in the wordnet dict, the default tag used is NOUN.\n","  return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_tweet])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BflzA7Ajg5tZ"},"outputs":[],"source":["def split_hash(tweet):\n","  # Find all hashtags in the tweet\n","  hashtags = re.findall(r'#\\w+', tweet)\n","  # Loop through each hashtag and split it into individual words\n","  for hashtag in hashtags:\n","      # Remove the \"#\" character from the hashtag\n","      hashtag_text = hashtag[1:]\n","      # Use regular expressions to split the hashtag into individual words based on CamelCase eg: 'ElectricVehcile' into 'Electric Vehicle'\n","      words = re.findall(r'[A-Z]?[a-z]+', hashtag_text)\n","      # Combine the words back into a single string and replace the original hashtag in the tweet with the new, split hashtag\n","      split_hashtag = ' '.join(words).lower()\n","      tweet = tweet.replace(hashtag, split_hashtag)\n","\n","  return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT4GWoOkhvt6"},"outputs":[],"source":["def remove_escape_sequence(tweet):\n","    # Remove escape sequences from tweet to avoid issues such avoid addition of extra spaces\n","    tweet = tweet.replace('\\n', '')\n","    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Wz_Z1u3FkNA"},"outputs":[],"source":["def preprocessing_tweets(tweet):\n","    # Preprocessing steps for sentiment analysis\n","    tweet = split_hash(tweet)\n","    tweet = remove_urls(tweet)\n","    tweet = remove_escape_sequence(tweet)\n","    tweet = html.unescape(tweet)\n","    tweet = expand_contractions(tweet)\n","    tweet = convert_emoticons(tweet)\n","    tweet = convert_emojis(tweet)\n","    tweet = tweet.replace('::',' ')\n","    tweet = replace_numbers(tweet)\n","    tweet = clean_mentions(tweet)\n","    tweet = replace_slangs_words(tweet, slang_dict)\n","    tweet = replace_nonstandard(tweet, nonstandard_dict)\n","    tweet = remove_punctuations(tweet)\n","    tweet = lemmatize_words(tweet)\n","    tweet = tweet.lower()\n","    tweet = remove_stopwords(tweet)\n","    return (tweet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyr8RpvDGI-B"},"outputs":[],"source":["dataset['text'] = dataset['text'].apply(lambda x:preprocessing_tweets(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9p3EINa2UNmT"},"outputs":[],"source":["dataset = dataset.dropna() # There may be tweets with only mentions therefore after preprocessing results in nan"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1679001997462,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"uf1SJyu6T1WK","outputId":"33cbb943-da7e-4f5d-d381-d292c660c7c8"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c1e50763-d33c-464f-b446-778904014c9f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>awww bummer get david carr third day wink smirk</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>upset cannot update facebook texting might cry...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>dive many time ball managed save rest go bound</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>whole body feel itchy like fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>behave mad cannot see</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>1</td>\n","      <td>wake school best feeling ever</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>1</td>\n","      <td>thewdbcom cool hear old walt interview</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>1</td>\n","      <td>ready mojo makeover ask detail</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>1</td>\n","      <td>happy th birthday boo alll time tupac amaru sh...</td>\n","    </tr>\n","    <tr>\n","      <th>1599999</th>\n","      <td>1</td>\n","      <td>happy charitytuesday</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1592240 rows Ã— 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1e50763-d33c-464f-b446-778904014c9f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c1e50763-d33c-464f-b446-778904014c9f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c1e50763-d33c-464f-b446-778904014c9f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         sentiment                                               text\n","0                0    awww bummer get david carr third day wink smirk\n","1                0  upset cannot update facebook texting might cry...\n","2                0     dive many time ball managed save rest go bound\n","3                0                    whole body feel itchy like fire\n","4                0                              behave mad cannot see\n","...            ...                                                ...\n","1599995          1                      wake school best feeling ever\n","1599996          1             thewdbcom cool hear old walt interview\n","1599997          1                     ready mojo makeover ask detail\n","1599998          1  happy th birthday boo alll time tupac amaru sh...\n","1599999          1                               happy charitytuesday\n","\n","[1592240 rows x 2 columns]"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zy_O-7Rp2bwf"},"outputs":[],"source":["dataset.to_csv('Preprocessed16.csv', index = False) # To be removed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBuFQuhVFTWm"},"outputs":[],"source":["# from collections import Counter\n","# cnt = Counter()\n","# for text in dataset[\"text\"].values:\n","#     for word in text.split():\n","#         cnt[word] += 1\n","\n","# cnt.most_common(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WMse2Iyfn8K"},"outputs":[],"source":["# sen =\"&quot; Hi My name is ðŸ¤£ðŸ˜‚ðŸ˜€â¤ðŸ˜ðŸ˜ðŸ‘ðŸ˜’ðŸ‘ŒðŸ¤·â€â™‚ðŸ˜ŠðŸ˜˜ :/ :)\\tðŸ˜„ ____ \\y\\n wouldn' t https://www.google.com \\t\\n saying\\t way can't #ElectricVehicle #EV #EEE @nfdwe \\n@kwenrf234 123 yolo can't wanna lawda harsh.&quot;\"\n","# print(preprocessing_tweets(sen))"]},{"cell_type":"markdown","metadata":{"id":"yX_sQ4WfUnRf"},"source":["# Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__HT0nOmUEr6"},"outputs":[],"source":["dataset = pd.read_csv('./Preprocessed16.csv') #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndXXvbOrFbaL"},"outputs":[],"source":["token = Tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyYNJHS9Lp9F"},"outputs":[],"source":["# Tokenize tweets and add unique words to the vocabulary\n","def getVocab(tweets):\n","  vocab = set()\n","  for tweet in tweets:\n","      words = word_tokenize(tweet)\n","      vocab.update(words)\n","  return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"op-CYRQXL-G2"},"outputs":[],"source":["vocab = getVocab(dataset['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxmtaVjnaOnn"},"outputs":[],"source":["# Sort vocab of tweet dataset\n","vocab = sorted(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679002315906,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"rf8MJxYaFgmq","outputId":"a51ce50c-f413-481d-ee1a-a8c96c8feec1"},"outputs":[{"data":{"text/plain":["391031"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab) #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0g-buIppDCKD"},"outputs":[],"source":["# Prune pre-trained embeddings\n","def optimise_glove(gloveText,vocab):\n","  # Takes dictionary of glove word vectors as input and returns a new dictionary containing only those words\n","  # that are present in a given set of unique words.\n","  newGlove= dict()\n","  for word, vector in gloveText.items():\n","    if word in vocab or word =='unk':\n","      newGlove[word] = gloveText[word]\n","  return newGlove"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIxbKGiPAj8a"},"outputs":[],"source":["def create_vocab_to_dict(words):\n","    # Convert vocabulary of tweet into dictionary having keys as words and values as index of that word\n","    vocab_to_dict = {}\n","    index = 0\n","    for word in words:\n","        if word not in vocab_to_dict:\n","            vocab_to_dict[word] = index\n","            index += 1\n","    return vocab_to_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZtdMIciAlY3"},"outputs":[],"source":["vocab_dict = create_vocab_to_dict(vocab)"]},{"cell_type":"markdown","metadata":{"id":"zyjMwOAIVfj5"},"source":["Use of a pre-trained Twitter GloVe model with embedding space of 100 from Stanford for Sentiment Analysis task on Twitter data.\n","> Pre-trained GloVe Model: Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)\n","\n","> Link: https://nlp.stanford.edu/projects/glove/\n","\n","> Note:For the glove file Download the file from: https://drive.google.com/file/d/1-6EjyzaKqXjlMZduUiC8-W9K4VI8PXxx/view?usp=share_link\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jK9RjasdAwtM"},"outputs":[],"source":["def creatingEmbeddingMatrix(vocab_dict):\n","  # To load pretrained glove word vectors file\n","  with open('./glove.twitter.27B.100d.txt','r',encoding='UTF-8') as f:\n","    glove_model = {}\n","    for line in f:\n","      split_line = line.split()\n","      word = split_line[0]\n","      embedding = np.array(split_line[1:], dtype=np.float32)\n","      glove_model[word] = embedding\n","    print(len(glove_model))\n","      # Removing words which are not present in the gloveText file\n","    opt_glove_model = optimise_glove(glove_model,vocab_dict)\n","      # Getting list of vocab(unique words) which are present in gloveText file\n","      # Creating matrix of 0s of (m,n) where,\n","      # m is the number of unique words which are present in gloveText file + 1 (Adding padding vector at start)\n","      # n is the embedding space which is of length 300.\n","    embedding_matrix = np.zeros((len(opt_glove_model)+1,100))\n","    i = 1\n","      # Creation of embedding matrix(vectors) of all the matching vocab words for embedding layer\n","    for word in list(opt_glove_model.keys()):\n","      embedding_matrix[i] = np.asarray(opt_glove_model[word], 'float32')\n","      i+=1\n","    embedding_matrix = np.asarray(embedding_matrix, 'float32')\n","    return embedding_matrix, opt_glove_model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":38847,"status":"ok","timestamp":1679003857339,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"64K2LeoGNu2c","outputId":"ebc20d65-97a1-4c12-f266-06bb78efa7a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["1193514\n"]}],"source":["embedding_matrix, pruned_glove_model = creatingEmbeddingMatrix(vocab_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1679003090162,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"C1CWcOkKz7kE","outputId":"801cdb72-1e77-4b88-b1a6-2753ed122457"},"outputs":[{"name":"stdout","output_type":"stream","text":["124539\n"]}],"source":["print(len(pruned_glove_model)) #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_DcKYBN7Vbh"},"outputs":[],"source":["# Save new pruned model to a file\n","with open(\"pruned_glove_model.txt\", \"w\", encoding=\"utf-8\") as f:\n","    for word, vector in pruned_glove_model.items():\n","        f.write(f\"{word} {' '.join(str(x) for x in vector)}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICmb6zcnE0v-"},"outputs":[],"source":["pruned_glove_model_dict = create_vocab_to_dict(list(pruned_glove_model))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1679003938857,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"FHzB0i0Fbs1v","outputId":"28de4bb8-64da-493b-c725-3cd5b48dee93"},"outputs":[{"data":{"text/plain":["124539"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["len(pruned_glove_model_dict) #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QQlLTryA3yn"},"outputs":[],"source":["max_tweet_len = 60\n","unk_index =  list(pruned_glove_model_dict).index('unk')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1679004028920,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"pJFoGvkccDqu","outputId":"ca2d3ae7-d38c-49ce-ddfc-b867e056e246"},"outputs":[{"data":{"text/plain":["29393"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["unk_index #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1679004040738,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"LUhwMrpvcGnI","outputId":"b02399c1-686b-4c35-9577-ebf2f81bb792"},"outputs":[{"data":{"text/plain":["29393"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["pruned_glove_model_dict['unk'] #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0EOXLhhfn8R"},"outputs":[],"source":["with open('Pruned_gloveModel.pkl', 'wb') as f:  # Open a text file\n","    pickle.dump(pruned_glove_model_dict, f) # Serialize the list\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vbwz1x_cCRI7"},"outputs":[],"source":["def getInput(tweets, words, max_tweet_len, unk_index):\n","    X = []\n","    for tweet in tqdm(tweets):\n","        idx = [words[word]+1  if word in words else unk_index+1 for word in tweet.split()]\n","        X.append(idx)\n","    X = pad_sequences(X, maxlen=max_tweet_len, padding='post', truncating='post', value=0)\n","    return X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15630,"status":"ok","timestamp":1679004131243,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"cIqGitGR78_2","outputId":"f6241124-223d-47dc-e37b-e2d97dd7bbe9"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1592240/1592240 [00:08<00:00, 183998.19it/s]\n"]}],"source":["X = getInput(dataset['text'], pruned_glove_model_dict, max_tweet_len, unk_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440,"status":"ok","timestamp":1679004141012,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"WTNJwSf9fnoo","outputId":"593d0c6f-7229-4ac2-b0a6-8ec5071769c7"},"outputs":[{"data":{"text/plain":["500    list item ebay sell takes forevermeanwhile col...\n","501            get presentation slide cry week hard week\n","502                                                 lose\n","503    like previously skin thing start season two li...\n","504             wish therei pretty good scar shit people\n","505                   going sound vain run fav lip gloss\n","506    never click link scream spider spider know wel...\n","507                                   time work get sick\n","508                           imac come indonesia states\n","509                    dammit need stop buying furniture\n","Name: text, dtype: object"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["dataset['text'][500:510] #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1679004166967,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"terRnDf4Ada6","outputId":"32a1be22-c16b-4453-dd1a-567322960657"},"outputs":[{"data":{"text/plain":["array([-7.5353e-02,  3.6746e-01,  1.5601e-01, -3.0391e-01,  6.6648e-01,\n","       -2.7182e-01,  7.0206e-01, -3.3533e-01, -1.4386e-01,  2.7116e-02,\n","        2.1675e-02, -1.0689e-01, -3.3964e+00,  2.7311e-01, -7.3037e-02,\n","        3.0913e-01,  4.0654e-01, -2.1790e-03,  7.8056e-03,  6.4235e-02,\n","       -2.9094e-01, -4.0283e-02, -3.1580e-01,  2.3444e-01, -5.6632e-01,\n","       -1.4728e-01,  3.6431e-01, -8.0864e-01,  9.0580e-01,  8.2974e-02,\n","       -8.3946e-02,  1.5435e-01, -4.3824e-01, -1.0304e-01,  7.6613e-01,\n","       -9.4312e-01,  1.0410e+00,  1.4602e-01, -1.6489e-01, -1.9296e-01,\n","       -3.1341e-02,  1.5007e-02,  1.7113e-01, -3.3630e-01,  1.3170e-01,\n","       -5.7878e-01, -6.8777e-02, -7.7445e-01, -3.2352e-01,  3.5709e-01,\n","       -1.0262e-02, -1.7327e-01, -5.7990e-01,  2.0623e-01, -2.0086e-01,\n","        8.2070e-01, -5.3029e-03, -1.6298e-01, -2.4454e-01, -2.0762e-01,\n","       -3.3225e-01, -7.6951e-02,  5.5174e-01,  2.5978e-01, -1.6672e-01,\n","        3.9690e-01, -2.3708e-01, -6.9310e-01, -8.9850e-02, -1.1997e-01,\n","       -3.0594e-01, -2.3998e-01,  1.6878e-02, -1.1610e-02, -2.3497e-01,\n","       -1.7235e-01, -4.4929e-01, -6.0207e-01, -1.6956e-01, -6.7533e-01,\n","        1.5018e+00,  1.4806e-01,  1.5759e-02, -2.7342e-01, -3.1868e-01,\n","        1.2116e-01, -2.1351e-01,  5.0634e-01,  2.9135e-01, -2.5777e-01,\n","       -4.5684e-01,  5.2214e-01, -4.2985e-01, -1.8759e-01,  2.1763e-01,\n","        4.2332e-01,  3.7589e-01,  5.1629e-01,  5.9594e-02, -4.2252e-01],\n","      dtype=float32)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["embedding_matrix[1435] #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679004186791,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"sjFC7FcEAihk","outputId":"1fba849b-cdbd-4b71-d5a3-dc96ccff3d1a"},"outputs":[{"data":{"text/plain":["array([-7.5353e-02,  3.6746e-01,  1.5601e-01, -3.0391e-01,  6.6648e-01,\n","       -2.7182e-01,  7.0206e-01, -3.3533e-01, -1.4386e-01,  2.7116e-02,\n","        2.1675e-02, -1.0689e-01, -3.3964e+00,  2.7311e-01, -7.3037e-02,\n","        3.0913e-01,  4.0654e-01, -2.1790e-03,  7.8056e-03,  6.4235e-02,\n","       -2.9094e-01, -4.0283e-02, -3.1580e-01,  2.3444e-01, -5.6632e-01,\n","       -1.4728e-01,  3.6431e-01, -8.0864e-01,  9.0580e-01,  8.2974e-02,\n","       -8.3946e-02,  1.5435e-01, -4.3824e-01, -1.0304e-01,  7.6613e-01,\n","       -9.4312e-01,  1.0410e+00,  1.4602e-01, -1.6489e-01, -1.9296e-01,\n","       -3.1341e-02,  1.5007e-02,  1.7113e-01, -3.3630e-01,  1.3170e-01,\n","       -5.7878e-01, -6.8777e-02, -7.7445e-01, -3.2352e-01,  3.5709e-01,\n","       -1.0262e-02, -1.7327e-01, -5.7990e-01,  2.0623e-01, -2.0086e-01,\n","        8.2070e-01, -5.3029e-03, -1.6298e-01, -2.4454e-01, -2.0762e-01,\n","       -3.3225e-01, -7.6951e-02,  5.5174e-01,  2.5978e-01, -1.6672e-01,\n","        3.9690e-01, -2.3708e-01, -6.9310e-01, -8.9850e-02, -1.1997e-01,\n","       -3.0594e-01, -2.3998e-01,  1.6878e-02, -1.1610e-02, -2.3497e-01,\n","       -1.7235e-01, -4.4929e-01, -6.0207e-01, -1.6956e-01, -6.7533e-01,\n","        1.5018e+00,  1.4806e-01,  1.5759e-02, -2.7342e-01, -3.1868e-01,\n","        1.2116e-01, -2.1351e-01,  5.0634e-01,  2.9135e-01, -2.5777e-01,\n","       -4.5684e-01,  5.2214e-01, -4.2985e-01, -1.8759e-01,  2.1763e-01,\n","        4.2332e-01,  3.7589e-01,  5.1629e-01,  5.9594e-02, -4.2252e-01],\n","      dtype=float32)"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["pruned_glove_model['list'] #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1679004156665,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"jopn1pyK9tk-","outputId":"da11b16d-f525-473c-bbbc-af7f71140494"},"outputs":[{"data":{"text/plain":["'list'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["list(pruned_glove_model)[1434] #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1679004145014,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"CKj8vZEw9nGL","outputId":"b91c8891-07bc-46f3-d3cb-449a6b0fd407"},"outputs":[{"data":{"text/plain":["array([[  1435,   5056,   6450, ...,      0,      0,      0],\n","       [    24,   8490,   5855, ...,      0,      0,      0],\n","       [   806,      0,      0, ...,      0,      0,      0],\n","       ...,\n","       [101154,   1155,   1587, ...,      0,      0,      0],\n","       [ 26174,     64,   1440, ...,      0,      0,      0],\n","       [   406,    418,    889, ...,      0,      0,      0]])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["X[500:520] #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xipfi7dDgnG"},"outputs":[],"source":["def getOutput(sentiments):\n","    sentiment_array = []\n","    for sentiment in sentiments:\n","        sentiment_array.append(sentiment)\n","    return np.array(sentiment_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGg_P_RAGGh3"},"outputs":[],"source":["Y = getOutput(dataset['sentiment'])"]},{"cell_type":"markdown","metadata":{"id":"VlxuqSX0dD_X"},"source":["# Data Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnqOkuvZGezl"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.20,shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"o_O1EpOtUue7"},"source":["# BiLSTM Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5310,"status":"ok","timestamp":1679004563260,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"QIbJMZGkOEkb","outputId":"4609b6dd-b041-422f-82b7-f7f7fa3f7a37"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 60, 100)           12454000  \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 60, 128)          84480     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128)              98816     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 12,637,425\n","Trainable params: 183,425\n","Non-trainable params: 12,454,000\n","_________________________________________________________________\n","None\n"]}],"source":["# Define the BiLSTM model\n","model = Sequential()\n","# Embedding Layer\n","model.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], input_length=X.shape[1], trainable=False))\n","# BiLSTM Layer\n","model.add(Bidirectional(LSTM(64, return_sequences = True)))\n","model.add(Bidirectional(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2)))\n","model.add(Dense(units=1, activation=\"sigmoid\"))\n","# Compile the model\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","# Print the model summary\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akS-7K7aZkW0","outputId":"68246583-c2d8-4557-ac09-da5fafb1a367"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","622/622 [==============================] - 6221s 10s/step - loss: 0.6549 - accuracy: 0.6055 - val_loss: 0.6375 - val_accuracy: 0.6231\n","Epoch 2/10\n","622/622 [==============================] - 5960s 10s/step - loss: 0.6342 - accuracy: 0.6289 - val_loss: 0.6124 - val_accuracy: 0.6537\n","Epoch 3/10\n","622/622 [==============================] - 6629s 11s/step - loss: 0.5939 - accuracy: 0.6703 - val_loss: 0.5725 - val_accuracy: 0.6921\n","Epoch 4/10\n","622/622 [==============================] - 7258s 12s/step - loss: 0.5670 - accuracy: 0.6960 - val_loss: 0.5636 - val_accuracy: 0.6986\n","Epoch 5/10\n","622/622 [==============================] - 7838s 13s/step - loss: 0.5521 - accuracy: 0.7093 - val_loss: 0.5550 - val_accuracy: 0.7072\n","Epoch 6/10\n","622/622 [==============================] - 8198s 13s/step - loss: 0.5426 - accuracy: 0.7175 - val_loss: 0.5483 - val_accuracy: 0.7123\n","Epoch 7/10\n","622/622 [==============================] - 8622s 14s/step - loss: 0.5373 - accuracy: 0.7223 - val_loss: 0.5331 - val_accuracy: 0.7262\n","Epoch 8/10\n","622/622 [==============================] - 9295s 15s/step - loss: 0.5317 - accuracy: 0.7269 - val_loss: 0.5287 - val_accuracy: 0.7293\n","Epoch 9/10\n","622/622 [==============================] - 9579s 15s/step - loss: 0.5281 - accuracy: 0.7299 - val_loss: 0.5247 - val_accuracy: 0.7319\n","Epoch 10/10\n","622/622 [==============================] - 9980s 16s/step - loss: 0.5236 - accuracy: 0.7333 - val_loss: 0.5256 - val_accuracy: 0.7329\n"]}],"source":["history = model.fit(x_train, y_train, batch_size=2048, epochs=10,validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7KxpEnLOEne"},"outputs":[],"source":["model.save(\"BiLSTM_model.h5\")"]},{"cell_type":"markdown","metadata":{"id":"J0ULOjn7n_4G"},"source":["# Fine Tune Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFnY1pfr8159"},"outputs":[],"source":["data_EV = pd.read_csv('./Tweets_EV_FineTune.csv',encoding = 'UTF-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1679004949068,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"Ec6zpNAu81a4","outputId":"7ae74a81-6382-44db-facc-86a8bf1a15ee"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Lithium-ion battery explosions are now the thi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>When an Electric Vehicle catches fire, it canâ€™...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The current Mini electric vehicle is delightfu...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Vietnamese EV maker VinFast remains optimistic...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Electric Eco car on fire problem burn damage h...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>194</th>\n","      <td>i dislike fully electric vehicles only because...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>And the truth is the electrical grid cannot su...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>IMO, Hydrogen is the way to go over EV for env...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>I wonder how many EV drivers dislike single us...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>All EVâ€™s need the option for a manual pre-cond...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>199 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                            tweet_text  sentiment\n","0    Lithium-ion battery explosions are now the thi...          0\n","1    When an Electric Vehicle catches fire, it canâ€™...          0\n","2    The current Mini electric vehicle is delightfu...          1\n","3    Vietnamese EV maker VinFast remains optimistic...          1\n","4    Electric Eco car on fire problem burn damage h...          0\n","..                                                 ...        ...\n","194  i dislike fully electric vehicles only because...          0\n","195  And the truth is the electrical grid cannot su...          0\n","196  IMO, Hydrogen is the way to go over EV for env...          0\n","197  I wonder how many EV drivers dislike single us...          0\n","198  All EVâ€™s need the option for a manual pre-cond...          0\n","\n","[199 rows x 2 columns]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["data_EV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1679005032168,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"F0hfsc6qf4FG","outputId":"c93e7b00-000d-4674-84c1-c9d6e5654be8"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 199 entries, 0 to 198\n","Data columns (total 2 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   tweet_text  199 non-null    object\n"," 1   sentiment   199 non-null    int64 \n","dtypes: int64(1), object(1)\n","memory usage: 3.2+ KB\n"]}],"source":["data_EV.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CZIvovw9XQ1"},"outputs":[],"source":["data_EV['tweet_text'] = data_EV['tweet_text'].apply(lambda x:preprocessing_tweets((x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679005053449,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"zo5gaFjd9Bfn","outputId":"572d9d14-de4d-45bd-f65f-18ab981fea2b"},"outputs":[{"data":{"text/plain":["0      lithiumion battery explosion third leading fir...\n","1      electric vehicle catch fire put water foam cau...\n","2      current mini electric vehicle delightful recen...\n","3      vietnamese ev maker vinfast remain optimistic ...\n","4      electric eco car fire problem burn damage hybr...\n","                             ...                        \n","194    dislike fully electric vehicle almost get hit ...\n","195    truth electrical grid cannot support massive i...\n","196    going hydrogen way go ev environmental reason ...\n","197    wonder many ev driver dislike single use plast...\n","198    ev need option manual preconditioning button d...\n","Name: tweet_text, Length: 199, dtype: object"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["data_EV['tweet_text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ESWwg9qfn8b"},"outputs":[],"source":["def test_output(tweet_text, max_tweet_len):\n","    X=[]\n","    for tweet in tqdm(tweet_text):\n","        X1 = tweet.split(' ')\n","        with open('Pruned_gloveModel.pkl', 'rb') as f:\n","            # Create an Unpickler object and call its load() method to unpickle the desired value\n","            unpickler = pickle.Unpickler(f)\n","            obj = unpickler.load()\n","            idx = [obj[word]+1  if word in obj.keys() else obj['unk']+1 for word in X1]\n","            X.append(idx)\n","    X = pad_sequences(X, maxlen=max_tweet_len, padding='post', truncating='post', value=0)\n","    return X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10330,"status":"ok","timestamp":1679005590006,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"bbeKuCgP-Kcf","outputId":"2297c909-16e3-48ec-98e5-8bf040c4e36e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:07<00:00, 25.49it/s]\n"]}],"source":["max_tweet_len = 60\n","X = test_output(data_EV['tweet_text'], max_tweet_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1679005674097,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"fHR1WKR0fn8c","outputId":"68eebfdd-9a7d-42b7-b0d4-dd5c96cb0db7"},"outputs":[{"data":{"text/plain":["array([[29394,  3365, 11930, ...,     0,     0,     0],\n","       [ 6581, 10152,  1327, ...,     0,     0,     0],\n","       [ 3755,  1960,  6581, ...,     0,     0,     0],\n","       ...,\n","       [   91, 36942,   122, ...,     0,     0,     0],\n","       [  813,   343,  8186, ...,     0,     0,     0],\n","       [ 8186,    64,  4550, ...,     0,     0,     0]])"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["X #TO BE REMOVED"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdUM-JzSAvRF"},"outputs":[],"source":["Y = getOutput(data_EV['sentiment'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47297,"status":"ok","timestamp":1679007298940,"user":{"displayName":"Text Mining","userId":"11155557353604629569"},"user_tz":0},"id":"NZAy7KXpfn8c","outputId":"fe963ed0-c25a-4dfb-e207-7cb078981709"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_input (InputLayer  [(None, 60)]             0         \n"," )                                                               \n","                                                                 \n"," embedding (Embedding)       (None, 60, 100)           12454000  \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 60, 128)          84480     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128)              98816     \n"," nal)                                                            \n","                                                                 \n"," flatten_1 (Flatten)         (None, 128)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 512)               66048     \n","                                                                 \n"," dense_5 (Dense)             (None, 128)               65664     \n","                                                                 \n"," dense_6 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 12,769,137\n","Trainable params: 131,841\n","Non-trainable params: 12,637,296\n","_________________________________________________________________\n","None\n"]}],"source":["# Load pre-trained model\n","pretrained_model = load_model('BiLSTM_model.h5')\n","for layer in pretrained_model.layers:\n","    layer.trainable = False\n","\n","# Replace the last layer\n","num_classes = 1\n","last_layer = pretrained_model.get_layer('bidirectional_1')\n","last_output = last_layer.output\n","x = layers.Flatten()(last_output)\n","x = layers.Dense(512, activation='leaky_relu')(x)\n","x = layers.Dense(128, activation='leaky_relu')(x)\n","x = layers.Dense(num_classes, activation='sigmoid')(x)\n","model = Model(pretrained_model.input, x)\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHg8XPsboOd5","outputId":"77bb2bf0-1aff-47a8-8b9e-96101d01749d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","7/7 [==============================] - 9s 113ms/step - loss: 0.6801 - accuracy: 0.5578\n","Epoch 2/10\n","7/7 [==============================] - 1s 107ms/step - loss: 0.6516 - accuracy: 0.6080\n","Epoch 3/10\n","7/7 [==============================] - 1s 106ms/step - loss: 0.6318 - accuracy: 0.6633\n","Epoch 4/10\n","7/7 [==============================] - 1s 107ms/step - loss: 0.6224 - accuracy: 0.6533\n","Epoch 5/10\n","7/7 [==============================] - 1s 102ms/step - loss: 0.6211 - accuracy: 0.6533\n","Epoch 6/10\n","7/7 [==============================] - 1s 100ms/step - loss: 0.6195 - accuracy: 0.6533\n","Epoch 7/10\n","7/7 [==============================] - 1s 102ms/step - loss: 0.6210 - accuracy: 0.6734\n","Epoch 8/10\n","7/7 [==============================] - 1s 102ms/step - loss: 0.7170 - accuracy: 0.5678\n","Epoch 9/10\n","7/7 [==============================] - 1s 99ms/step - loss: 0.6641 - accuracy: 0.5879\n","Epoch 10/10\n","7/7 [==============================] - 1s 96ms/step - loss: 0.6582 - accuracy: 0.6281\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1f84a943eb0>"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["# Compile the model\n","num_epochs = 10\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X,Y, epochs=num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgRvz1TEoeXT"},"outputs":[],"source":["# Save the fine-tuned model\n","model.save('fine_tuned_model.h5')"]}],"metadata":{"colab":{"collapsed_sections":["AyociNMdz5YD"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}